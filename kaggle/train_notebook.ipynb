{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DYLUMO - Image to Music Training (Kaggle GPU)\n",
        "\n",
        "This notebook trains the FastMLP model to map image features to music features.\n",
        "\n",
        "**Architecture:** CLIP Image Features (512-dim) → FastMLP → Spotify Audio Features (13-dim)\n",
        "\n",
        "**Run this on Kaggle with GPU enabled!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install open-clip-torch faiss-cpu huggingface_hub datasets pyarrow -q\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Audio features\n",
        "AUDIO_FEATURES = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', \n",
        "                  'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
        "                  'duration_ms', 'time_signature']\n",
        "\n",
        "# Load Spotify data\n",
        "spotify_df = pd.read_csv('/kaggle/input/spotify-1million-tracks/spotify_data.csv')\n",
        "print(f'Loaded {len(spotify_df):,} tracks')\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "features_norm = scaler.fit_transform(spotify_df[AUDIO_FEATURES])\n",
        "\n",
        "# Map to emotions\n",
        "def map_emotion(v, e):\n",
        "    if v >= 0.5:\n",
        "        return 'excitement' if e >= 0.7 else 'amusement' if e >= 0.5 else 'contentment'\n",
        "    return 'anger' if e >= 0.7 else 'fear' if e >= 0.5 else 'sadness'\n",
        "\n",
        "spotify_df['emotion'] = spotify_df.apply(lambda r: map_emotion(r['valence'], r['energy']), axis=1)\n",
        "for i, c in enumerate(AUDIO_FEATURES):\n",
        "    spotify_df[f'{c}_norm'] = features_norm[:, i]\n",
        "\n",
        "print(spotify_df['emotion'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download EMID from HuggingFace\n",
        "from huggingface_hub import list_repo_files, hf_hub_download\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "os.makedirs('/kaggle/working/emid/images', exist_ok=True)\n",
        "os.makedirs('/kaggle/working/emid/raw', exist_ok=True)\n",
        "\n",
        "files = list_repo_files('ecnu-aigc/EMID', repo_type='dataset')\n",
        "parquet_files = [f for f in files if f.endswith('.parquet')]\n",
        "\n",
        "for pf in tqdm(parquet_files, desc='Downloading EMID'):\n",
        "    hf_hub_download(repo_id='ecnu-aigc/EMID', repo_type='dataset', filename=pf, local_dir='/kaggle/working/emid/raw')\n",
        "\n",
        "hf_hub_download(repo_id='ecnu-aigc/EMID', repo_type='dataset', filename='EMID_data.csv', local_dir='/kaggle/working/emid')\n",
        "print('Download complete!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract images from parquet\n",
        "images_dir = Path('/kaggle/working/emid/images')\n",
        "saved = 0\n",
        "for pq_file in tqdm(list(Path('/kaggle/working/emid/raw/data').glob('*.parquet'))):\n",
        "    df = pq.read_table(pq_file).to_pandas()\n",
        "    for idx, row in df.iterrows():\n",
        "        for col in ['Image1_filename', 'Image2_filename', 'Image3_filename']:\n",
        "            try:\n",
        "                d = row[col]\n",
        "                if d and 'bytes' in d and d['bytes']:\n",
        "                    fn = os.path.basename(d.get('path', f'{col}_{idx}.jpg'))\n",
        "                    p = images_dir / fn\n",
        "                    if not p.exists():\n",
        "                        p.write_bytes(d['bytes'])\n",
        "                        saved += 1\n",
        "            except: pass\n",
        "print(f'Extracted {saved} images')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare EMID metadata\n",
        "emid_df = pd.read_csv('/kaggle/working/emid/EMID_data.csv')\n",
        "img_data = []\n",
        "for _, row in emid_df.iterrows():\n",
        "    for i in [1,2,3]:\n",
        "        fn, tag = row[f'Image{i}_filename'], row[f'Image{i}_tag']\n",
        "        if pd.notna(fn): img_data.append({'filename': fn, 'emotion': tag})\n",
        "\n",
        "images_df = pd.DataFrame(img_data).drop_duplicates('filename')\n",
        "images_df['exists'] = images_df['filename'].apply(lambda x: (images_dir/x).exists())\n",
        "images_df = images_df[images_df['exists']].reset_index(drop=True)\n",
        "print(f'{len(images_df)} images ready')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract CLIP features\n",
        "import open_clip\n",
        "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "clip_model = clip_model.to(device).eval()\n",
        "\n",
        "all_feats = []\n",
        "for i in tqdm(range(0, len(images_df), 64), desc='CLIP'):\n",
        "    batch = images_df.iloc[i:i+64]\n",
        "    imgs = [clip_preprocess(Image.open(images_dir/r['filename']).convert('RGB')) for _,r in batch.iterrows()]\n",
        "    with torch.no_grad():\n",
        "        t = torch.stack(imgs).to(device)\n",
        "        f = clip_model.encode_image(t)\n",
        "        f = f / f.norm(dim=-1, keepdim=True)\n",
        "        all_feats.append(f.cpu().numpy())\n",
        "\n",
        "image_features = np.vstack(all_feats).astype(np.float32)\n",
        "print(f'Features: {image_features.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training pairs\n",
        "norm_cols = [f'{c}_norm' for c in AUDIO_FEATURES]\n",
        "X, y = [], []\n",
        "for idx, row in tqdm(images_df.iterrows(), total=len(images_df)):\n",
        "    songs = spotify_df[spotify_df['emotion'] == row['emotion']]\n",
        "    if len(songs) == 0: continue\n",
        "    for _, s in songs.sample(min(5, len(songs)), random_state=42+idx).iterrows():\n",
        "        X.append(image_features[idx])\n",
        "        y.append(s[norm_cols].values.astype(np.float32))\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "print(f'Pairs: {len(X)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "n = len(X)\n",
        "idx = np.random.permutation(n)\n",
        "train_X, train_y = X[idx[:int(0.8*n)]], y[idx[:int(0.8*n)]]\n",
        "val_X, val_y = X[idx[int(0.8*n):int(0.9*n)]], y[idx[int(0.8*n):int(0.9*n)]]\n",
        "test_X, test_y = X[idx[int(0.9*n):]], y[idx[int(0.9*n):]]\n",
        "print(f'Train: {len(train_X)}, Val: {len(val_X)}, Test: {len(test_X)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FastMLP Model\n",
        "class FastMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(512, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(128, 13)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "model = FastMLP().to(device)\n",
        "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training\n",
        "train_loader = DataLoader(TensorDataset(torch.FloatTensor(train_X), torch.FloatTensor(train_y)), batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(torch.FloatTensor(val_X), torch.FloatTensor(val_y)), batch_size=128)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    t_loss = sum(criterion(model(bx.to(device)), by.to(device)).item() for bx, by in train_loader) / len(train_loader)\n",
        "    for bx, by in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        criterion(model(bx.to(device)), by.to(device)).backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        v_loss = sum(criterion(model(bx.to(device)), by.to(device)).item() for bx, by in val_loader) / len(val_loader)\n",
        "    \n",
        "    scheduler.step(v_loss)\n",
        "    if epoch % 10 == 0: print(f'Epoch {epoch}: train={t_loss:.4f}, val={v_loss:.4f}')\n",
        "    if v_loss < best_loss:\n",
        "        best_loss = v_loss\n",
        "        torch.save(model.state_dict(), '/kaggle/working/best_model.pt')\n",
        "\n",
        "print(f'Best val loss: {best_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save artifacts\n",
        "model.load_state_dict(torch.load('/kaggle/working/best_model.pt'))\n",
        "torch.save({'model_state_dict': model.state_dict(), 'input_dim': 512, 'output_dim': 13, \n",
        "            'hidden_dims': [512, 256, 128]}, '/kaggle/working/dylumo_model.pt')\n",
        "\n",
        "with open('/kaggle/working/spotify_scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "np.save('/kaggle/working/spotify_features.npy', spotify_df[norm_cols].values.astype(np.float32))\n",
        "spotify_df[['track_id', 'track_name', 'artist_name', 'popularity', 'emotion']].to_parquet('/kaggle/working/spotify_metadata.parquet')\n",
        "\n",
        "print('Saved: dylumo_model.pt, spotify_scaler.pkl, spotify_features.npy, spotify_metadata.parquet')\n",
        "print('Download these files and place in checkpoints/ folder!')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
